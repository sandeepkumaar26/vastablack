# Vastablack AI Research Assistant

An advanced AI-powered research assistant that combines Retrieval-Augmented Generation (RAG), web search, and deep web scraping capabilities to provide comprehensive, well-researched answers to complex queries.

## ğŸŒŸ Features

- **RAG-based Knowledge Base**: Upload and query PDF documents using vector embeddings and semantic search
- **Web Search Integration**: Quick web searches using DuckDuckGo for current information
- **Deep Web Research**: Automated web scraping and content extraction from multiple sources
- **Multi-Model Support**: 
  - Google Gemini (cloud-based, recommended)
  - Ollama (local AI models like Mistral-Nemo)
- **Modern Tech Stack**: Built with LangGraph, LangChain, FastAPI, and Streamlit
- **Vector Database**: Powered by Qdrant for efficient semantic search
- **Interactive UI**: Clean Streamlit interface for easy interaction

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚
â”‚  (Frontend)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚
â”‚  (Backend API)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LangGraph Agent            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tools:                  â”‚   â”‚
â”‚  â”‚  â€¢ Knowledge Base (RAG)  â”‚   â”‚
â”‚  â”‚  â€¢ Web Search            â”‚   â”‚
â”‚  â”‚  â€¢ Deep Web Scraper      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qdrant Vector  â”‚
â”‚    Database     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- One of the following:
  - Google Gemini API key (recommended, free tier available)
  - Ollama with installed models (for local AI)

## ğŸš€ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/sandeepkumaar26/vastablack.git
   cd vastablack
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv .venv
   
   # Windows
   .venv\Scripts\activate
   
   # Linux/Mac
   source .venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r backend/requirements.txt
   ```

## âš™ï¸ Configuration

### Option 1: Using Google Gemini (Recommended)

1. Get a free API key from [Google AI Studio](https://aistudio.google.com/app/apikey)
2. Create a `.env` file in the project root:
   ```env
   GOOGLE_API_KEY=your_api_key_here
   ```

### Option 2: Using Ollama (Local AI)

1. Install Ollama from [ollama.com](https://ollama.com/download)
2. Pull a model:
   ```bash
   ollama pull mistral-nemo
   ```
3. No API key needed - the system will automatically use Ollama

## ğŸ¯ Usage

### Starting the Backend Server

```bash
python -m uvicorn backend.api.server:app --reload --host 127.0.0.1 --port 8000
```

The API will be available at:
- API: http://127.0.0.1:8000
- API Docs: http://127.0.0.1:8000/docs

### Starting the Frontend

```bash
streamlit run frontend/app.py
```

The Streamlit UI will be available at http://localhost:8501

### Using the Application

1. Open http://localhost:8501 in your browser
2. Type your question in the chat interface
3. The AI agent will:
   - Check the knowledge base for relevant information
   - Search the web if needed
   - Scrape and read website content for comprehensive answers
   - Provide sourced, well-researched responses

## ğŸ“ Project Structure

```
vastablack/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ agent.py          # Main LangGraph agent
â”‚   â”‚   â”œâ”€â”€ tools.py          # RAG knowledge base tool
â”‚   â”‚   â”œâ”€â”€ web_search.py     # Web search tool
â”‚   â”‚   â””â”€â”€ deep_web.py       # Deep web scraping tool
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.py         # FastAPI server
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ text_embeddings.py # Embedding models
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py     # PDF document processing
â”‚   â”‚   â””â”€â”€ text_chunker.py   # Text chunking
â”‚   â”œâ”€â”€ qdrant/
â”‚   â”‚   â”œâ”€â”€ client.py         # Qdrant client
â”‚   â”‚   â””â”€â”€ lc_bridge.py      # LangChain-Qdrant bridge
â”‚   â”œâ”€â”€ utils.py              # Utility functions
â”‚   â””â”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                # Streamlit UI
â”œâ”€â”€ .env.example              # Environment variables template
â”œâ”€â”€ .gitignore                # Git ignore rules
â””â”€â”€ README.md                 # This file
```

## ğŸ› ï¸ Technologies Used

- **AI/ML Frameworks**:
  - LangGraph - Agent orchestration
  - LangChain - LLM integration
  - Sentence Transformers - Text embeddings
  
- **LLM Providers**:
  - Google Gemini (via langchain-google-genai)
  - Ollama (via langchain-ollama)
  
- **Vector Database**:
  - Qdrant - Vector storage and similarity search
  
- **Backend**:
  - FastAPI - REST API framework
  - Uvicorn - ASGI server
  
- **Frontend**:
  - Streamlit - Interactive web UI
  
- **Web Tools**:
  - DDGS - DuckDuckGo search
  - BeautifulSoup4 - Web scraping
  - Requests - HTTP client

## ğŸ”§ API Endpoints

### POST /chat
Send a query to the AI agent

**Request:**
```json
{
  "query": "What is quantum computing?"
}
```

**Response:**
```json
{
  "query": "What is quantum computing?",
  "response": "Quantum computing is..."
}
```

### GET /
Health check endpoint

**Response:**
```json
{
  "status": "active",
  "message": "Convolve AI System is Online"
}
```

## ğŸ“ Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `GOOGLE_API_KEY` | Google Gemini API key | No* |
| `GEMINI_API_KEY` | Alternative for Google API key | No* |

*One of the API keys is required unless using Ollama

## ğŸ› Troubleshooting

### Error 500: Model not found

**Problem**: The system can't find the AI model.

**Solutions**:
1. If using Google Gemini: Set your `GOOGLE_API_KEY` in `.env`
2. If using Ollama: 
   - Check Ollama is running: `ollama list`
   - Pull the model: `ollama pull mistral-nemo`

### Qdrant lock error

**Problem**: `Storage folder ./qdrant_data is already accessed by another instance`

**Solution**: Stop all Python processes and restart the backend server

### Connection refused on port 8000

**Problem**: Backend server is not running.

**Solution**: Start the backend with:
```bash
python -m uvicorn backend.api.server:app --reload --host 127.0.0.1 --port 8000
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ‘¤ Author

**Sandeep Kumar**
- GitHub: [@sandeepkumaar26](https://github.com/sandeepkumaar26)

## ğŸ™ Acknowledgments

- Google for Gemini AI
- Ollama for local LLM support
- LangChain and LangGraph teams
- Qdrant for vector database
- Streamlit for the amazing UI framework

---

Made with â¤ï¸ using LangGraph and modern AI technologies
